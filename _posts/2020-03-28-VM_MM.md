---
title: Virtual Memory and Memory Mapping
date: 2020-03-28 20:58:53 
category: 
- 计算机原理
tags: 
- CSAPP 
- memory
- operating system
---
# Virtual Memory

Virtual memory was introduced in 1959. The main goal was to significantly simplify programming by abstracting the quirks of physical memory's hierarchy. Nowadays, virtual memory management is an essential component in almost every operating system's kernel. Today, I discuss the main concepts of virtual memory and its advantages.

## Real Addressing and Virtual Addressing

In the olden days, a process would access physical memory directly. This model, known as real addressing, is still in use in embedded systems and some PDAs. It's also used in DOS. However, real addressing mode is problematic for several reasons: 

- System stability. A process may accidentally tread on another process's memory. This can occur, for instance, when a process tries to write to an uninitialized pointer whose value belongs to the address space of a different process. Worse yet, a process could overwrite critical kernel code, thus freezing the system.

- Security. Apart from accidental trespassing, real addressing enables a process to steal and modify other processes' memory.

- Performance. In real mode addressing, swapping and memory mapping (I'll explain these techniques shortly) are usually unsupported. Consequently, the system isn't scalable. Crashes and freezes are common annoyances in such systems.

## How Virtual Addressing Works

In virtual addressing mode, every program is allocated a private address space that is completely isolated from other processes' address spaces. For example, if address 0x4000aef6 in process A points to a certain variable, the same address in process B may point to some other data variable or it may not exist in B. A virtual memory address is, therefore, an alias; the physical location to which it points is hidden from the process and from the programmer. For example, address 0x4000aef6 can be mapped to the physical address 0x102788ff in the system's RAM, or to offset 0 from cluster 1740 on the system's hard disk. Under this model, each process sees the system's memory as if it were the sole process running on the machine.

Early virtual memory managers used kernel modules to translate a virtual address to a physical address and vice versa. Today, this mapping is performed by special hardware, so it's completely transparent.

Virtual addressing enables the kernel to load and unload memory dynamically. A frequently-used code section can be stored in the RAM, whereas other sections of the program can be mapped to a swap file. When the process accesses a memory address that is mapped to a swap file, the kernel reloads that portion to the system's RAM. From a user's point of view, there's no difference whatsoever between the two (except performance, of course).

## Memory Mapping

Memory mapping is a common concept in POSIX and Windows systems. It enables a process to map disk files to its address space, treating them as memory blocks rather than files. Usually, the mapping consists of translating a disk location to a virtual address and vice versa; there is no copying of disk data into the RAM. Memory mapping has several uses:

- Dynamic loading. By mapping executable files and shared libraries into its address space, a program can load and unload executable code sections dynamically.
- Fast File I/O. When you call file I/O functions, such as read() and write(), the data is copied to a kernel's intermediary buffer before it is transferred to the physical file or the process. This intermediary buffering is slow and expensive. Memory mapping eliminates this intermediary buffering, thereby improving performance significantly.
- Streamlining file access. Once you map a file to a memory region, you access it via pointers, just as you would access ordinary variables and objects.
- Memory persistence. Memory mapping enables processes to share memory sections that persist independently of the lifetime of a certain process.

The POSIX *<sys/mman.h>* header includes memory mapping syscalls and data structures. Because this interface is more intuitive and simpler than that of Windows, I base my memory mapping example on the POSIX library.

The *mmap()* system call:

```C
caddr_t mmap(caddress_t map_addr,
       size_t length,
       int protection,
       int flags,
       int fd,
       off_t offset);
```
Let's examine what each parameter means.

*map_addr* is the address to which the memory should be mapped. A NULL value allows the kernel to pick any address (normally you'd use this value). length contains the number of bytes to map from the file. protection indicates the types of access allowed to the mapped region:

*flags* contains various mapping attributes; for instance, MAP_LOCKED guarantees that the mapped region is never swapped.

*fd* is the mapped file's descriptor.

Finally, *offset* specifies the position in the file from which mapping should begin, with offset 0 indicating the file's beginning.

In the following example, the program maps the first 4 KB of a file passed in command line into its memory and then reads *int* value from it:

```C
#include <errno.h>
#include <fcntl.h>
#include <sys/mman.h>
#include <sys/types.h>

 int main(int argc, char *argv[])
 {
 int fd;
 void * pregion;
 if (fd= open(argv[1], O_RDONLY) <0)
 {
 perror("failed on open");
 return –1;
 }
 /*map first 4 kilobytes of fd*/
 pregion=mmap(NULL, 4096, PROT_READ,MAP_SHARED,fd,0);
 if (pregion==(caddr_t)-1)
 {
 perror("mmap failed")
 return –1;
 }
 close(fd); //close the physical file because we don't need it
 //access mapped memory; read the first int in the mapped file
 int val= *((int*) pregion);
}
```

To **unmap** a mapped region, use the *munmap()* function:

```C
int munmap(caddr_t addr, int length);
```

*addr* is the address of the region being unmapped. *length* specifies how much of the memory should be unmapped (you may unmap a portion of a previously-mapped region). The following example unmaps the first kilobyte of the previously-mapped file. The remaining three KB still remain mapped to the process's RAM after this call:
```C
munmap(pregion, 1024);
```
## Summary

Without virtual addressing, programming would be much more difficult than you'd imagine. Furthermore, many common programming concepts such as dynamic memory allocation and multiple processing would be harder to implement. Virtual addressing thus proves once more that "there's no problem that can't be solved by an additional level of indirection."

 
# Dynamic Memory Allocation and Virtual Memory

Every application running on your operating system has its unique address space, which it sees as a continuous block of memory. In fact the memory is not physically continuous (it is fragmented), this is just the impression the operating system gives to every program and it's called virtual memory. The size of the virtual memory is the maximum size of the maximum size your computer can address using pointers (usually on a 32-bit processor each process can address 4 GB of memory). The natural question that arises is what happens when a process wants to access more memory than your machine physically has available as RAM? Due to having a virtual address space, parts of the hard disk can be mapped together with real memory and the process doesn't have to know anything about whether the address is physically stored in RAM or on the hard disk. The operating system maintains a table, where virtual addresses are mapped with their correspondent physical addresses, which is used whenever a request is made to read or write to a memory address.


Typically, in each process, the virtual memory available to that process is called its **address space**. Each process's address space is typically organized in 6 sections that are illustrated in the next picture: environment section - used to store environment variables and command line arguments; the stack, used to store memory for function arguments, return values, and automatic variables; the heap (free store) used for dynamic allocation, two data sections (for initialized and uninitialized static and global variables) and a text section where the actual code is kept.

![memorg.png](https://s1.ax1x.com/2020/03/28/GAsF39.png)

## The Heap
To understand why the dynamic memory allocation is time consuming let's take a closer look at what is actually happening. The memory area where new gets its blocks of memory for allocation (usually called free store or heap) is illustrated in the following picture:
![heap.png](https://s1.ax1x.com/2020/03/28/GAs3jI.png)

When *new* is invoked, it starts looking for a free memory block that fits the size for your request. Supposing that such a block of memory is found, it is marked as reserved and a pointer to that location is returned. There are several algorithms to accomplish this because a compromise has to be made between scanning the whole memory for finding the smallest free block bigger than the size of your object, or returning the first one where the memory needed fits. In order to improve the speed of getting a block of memory, the free and reserved areas of memory are maintained in a data structure similar to binary trees called a heap. The various algorithms for finding free memory are beyond the scope of this article and you can find a thorough discussion about them in D. Knuth's monograph The Art of Computer Programming -- Vol.1, Fundamental Algorithms). This overhead combined with the risk for memory leaks makes the use of automatic memory (allocated on the stack) preferred whenever possible and the allocation is not large.

## How much Virtual Memory do you get

Even though every application has its own 4 GB (on 32-bit systems) of virtual memory, that does not necessarily mean that your program can actually use all of that memory. For example, on Windows, the upper 2 GB of that memory are allocated to the operating system kernel, and are unavailable to the process. (Therefore, any pointer starting with 0x8xxxxxxx is unavailable in user space.) On Linux, the upper 1 GB is kernel address space. Typically, operating systems provide means for changing these defaults (such as the /3GB switch on Windows. It is rare, however, that you really want or need to do so.

## Address Space Fragmentation

Another concern with memory allocation is that if you allocate memory in non-contiguous blocks, over time "holes" will develop. For example, if you allocate 10 KB and it is taken from the middle of a 20 MB chunk of memory, then you can no longer allocate that 20 MB a one chunk of memory. Doing this enough times will cause you to no longer be able to allocate 20 MB at once. This can cause allocation failures even when there is free memory. Note that this is true even with virtual memory because what matters is that you need a continuous block of addresses, not a continuous block of physical memory.

One way to address this problem is to avoid doing things that have problems due to fragmentation, such as avoiding large allocations--anything more than a few tens of MB is certainly asking for trouble. Second, many heap implementations help you with this already by allocating a large chunk of virtual address space and carving it up for you (usually the heap allocates address space from the operating system and then provides smaller chunks when requested). But if you know that you will have a class that has a lot of small instances, you could overload operator new and preallocate a large continuous chunk of memory, splitting off small pieces for each class from that chunk.

---

**Reference**

[Dynamic Memory Allocation and Virtual Memory](https://www.cprogramming.com/tutorial/virtual_memory_and_heaps.html)

## Overview

Cache Memory is a special very high-speed memory. It is used to speed up and synchronizing with high-speed CPU. Cache memory is costlier than main memory or disk memory but economical than CPU registers. Cache memory is an extremely fast memory type that acts as a buffer between RAM and the CPU. It holds frequently requested data and instructions so that they are immediately available to the CPU when needed.

Cache memory is used to reduce the average time to access data from the Main memory. The cache is a smaller and faster memory which stores copies of the data from frequently used main memory locations. There are various different independent caches in a CPU, which store instructions and data.

## Cache Performance

When the processor needs to read or write a location in main memory, it first checks for a corresponding entry in the cache.

- If the processor finds that the memory location is in the cache, a cache hit has occurred and data is read from cache
- If the processor does not find the memory location in the cache, a cache miss has occurred. For a cache miss, the cache allocates a new entry and copies in data from main memory, then the request is fulfilled from the contents of the cache.

The performance of cache memory is frequently measured in terms of a quantity called Hit ratio.

```
Hit ratio = hit / (hit + miss) =  no. of hits/total accesses
```
We can improve Cache performance using higher cache block size, higher associativity, reduce miss rate, reduce miss penalty, and reduce Reduce the time to hit in the cache.
## Cache organization 
Consider a computer system where each memory address has m bits that form $M = 2^m$ unique addresses. As illustrated in Figure below, a cache for such a machine is organized as an array of $S = 2^s$ cache sets. Each set consists of E cache lines. Each line consists of a data block of $B = 2^b$ bytes, a valid bit that indicates whether or not the line contains meaningful information, and $t = m − (b + s)$ tag bits (a subset of the bits from the current block’s memory address) that uniquely identify the block stored in the cache line. In general, a cache’s organization can be characterized by the tuple $(S, E, B, m)$. The size (or capacity) of a cache, C, is stated in terms of the aggregate size of all the blocks. The tag bits and valid bit are not included. Thus, $C = S × E × B$.


![GKx2lt.png](https://s1.ax1x.com/2020/03/31/GKx2lt.png)
The parameters are summarized as below.
![GMetsg.png](https://s1.ax1x.com/2020/03/31/GMetsg.png)
## Cache Mapping
Cache mapping is kind of like hash algorithm. It is used to determine which block of cache memory that a certain memory address should be put into.
There are three different types of mapping used for the purpose of cache memory depending on the *E* or number of cache lines per set, which are as follows: Direct mapping (E = 1), Set-Associative mapping(1 < E < C/B), and  Fully Associative mapping (E = C/B). These are explained below.

1. **Direct Mapping**  
   The simplest technique, known as direct mapping, maps each block of main memory into only one possible cache line($E = 1$, so each group has only one line). 
   
   The process that a cache goes through of determining whether a request is a hit or a miss and then extracting the requested word consists of three steps: *(1) set selection, (2) line matching, and (3) word extraction.*

   ![GMEIW8.png](https://s1.ax1x.com/2020/03/31/GMEIW8.png)




2. **Set-associative Mapping**   
for cache with E that 1< E < C/B, we usually call it E-way associative caches.
This form of mapping is an enhanced form of direct mapping where the drawbacks of direct mapping are removed (in terms to cache conflicts). Set associative addresses the problem of possible thrashing in the direct mapping method. It does this by saying that instead of having exactly one line that a block can map to in the cache, we will group a few lines together creating a set. Then a block in memory can map to any one of the lines of a specific set..Set-associative mapping allows that each word that is present in the cache can have two or more words in the main memory for the same index address. Set associative cache mapping combines the best of direct and fully associative cache mapping techniques.
![GMma6O.png](https://s1.ax1x.com/2020/03/31/GMma6O.png)


3. **Fully Associative Mapping**  
In this type of mapping, the associative memory is used to store content and addresses of the memory word. Any block can go into any line of the cache (There are only one set). This means that the word id bits are used to identify which word in the block is needed, but the tag becomes all of the remaining bits. This enables the placement of any word at any place in the cache memory. It is considered to be the fastest and the most flexible mapping form. However, it is very expensive to implemented this in hardware, so the size of this type of cache are usually small, such as TLB(Translation Lookaside Buffer) in virtual memory system. 
![GMmhng.png](https://s1.ax1x.com/2020/03/31/GMmhng.png)


## Write Strategy

Suppose we write a word w that is already cached (a write hit). After the cache updates its copy of w, what does it do about updating the copy of w in the next lower level of the hierarchy? The simplest approach, known as *write-through*, is to immediately write w’s cache block to the next lower level. While simple, write-through has the disadvantage of causing bus traffic with every write.   
Another approach, known as *write-back*, defers the update as long as possible by writing the updated block to the next lower level only when it is evicted from the cache by the replacement algorithm. Because of locality, write-back can significantly reduce the amount of bus traffic, but it has the disadvantage of additional complexity. The cache must maintain an additional *dirty bit* for each cache line that indicates whether or not the cache block has been modified.   
Another issue is how to deal with write misses. One approach, known as *writeallocate*, loads the corresponding block from the next lower level into the cache and then updates the cache block. Write-allocate tries to exploit spatial locality of writes, but it has the disadvantage that every miss results in a block transfer from the next lower level to the cache. The alternative, known as no-write-allocate, bypasses the cache and writes the word directly to the next lower level. Writethrough caches are typically no-write-allocate. Write-back caches are typically write-allocate.


## Locality 

Since size of cache memory is less as compared to main memory. So to check which part of main memory should be given priority and loaded in cache is decided based on locality of reference.

Locality is typically described as having two distinct forms: temporal locality and spatial locality. In a program with good temporal locality, a memory location that is referenced once is likely to be referenced again multiple times in the near future. In a program with good spatial locality, if a memory location is referenced once, then the program is likely to reference a nearby memory location in the near future.

 - Programs that repeatedly reference the same variables enjoy good temporal locality. . For programs with stride-k reference patterns, the smaller the stride, the better the spatial locality. 
 - Programs with stride-1 reference patterns have good spatial locality. Programs that hop around memory with large strides have poor spatial locality. 
 - Loops have good temporal and spatial locality with respect to instruction fetches. The smaller the loop body and the greater the number of loop iterations, the better the locality.


**Reference**  
This passage is adopted from [Cache Memory in Computer Organization](https://www.geeksforgeeks.org/cache-memory-in-computer-organization/)
and CSAPP(3rd).